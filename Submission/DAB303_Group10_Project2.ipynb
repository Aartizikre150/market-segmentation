{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Group No. 10**\n",
        "\n",
        "Aarti Anil Zikre\n",
        "\n",
        "Andrews Truman\n",
        "\n",
        "Premkumar Janakbhai Patel\n",
        "\n",
        "Vitthlesh Sheth"
      ],
      "metadata": {
        "id": "fNYycR6OsB1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:** To understand and gain insights from a retail dataset by performing various exploratory data analyses, data visualization, and data modelling.\n",
        "\n",
        "**Dataset Columns:**\n",
        "\n",
        "- **InvoiceNo:** Invoice number. A unique number per invoice.\n",
        "- **StockCode:** Product code. A unique number per product.\n",
        "- **Description:** Product description.\n",
        "- **Quantity:** The number of products sold per invoice.\n",
        "- **InvoiceDate:** The date and time of the invoice.\n",
        "- **UnitPrice:** The price of one unit of the product.\n",
        "- **CustomerID:** Customer identification number.\n",
        "- **Country:** The country where the customer resides.\n"
      ],
      "metadata": {
        "id": "Fqe-EeLHr2B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing and Cleaning:\n"
      ],
      "metadata": {
        "id": "EbpGa4FkswtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1. Import necessary libraries and read the dataset:"
      ],
      "metadata": {
        "id": "AWW2vVrAszJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To get started, we'll first need to import the required libraries and load the dataset we plan to work with. The 'InvoiceDate' attribute is currently in a format we don't want, so we'll need to change it to our preferred format. Additionally, we've noticed some spelling errors and extra spaces in our text data, which we want to clean up. For the next steps, we'd like all our text data to be in uppercase."
      ],
      "metadata": {
        "id": "uP6mRZ8ct9R1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "togP5R-9ruHg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/DAB303/Project2/Sales_data.csv', encoding='unicode_escape', dtype= {'CustomerID': 'Int64'})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2. Display the top 10 rows of the dataframe:"
      ],
      "metadata": {
        "id": "tEUJK0vms0dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "BvVS8phZseXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3. Check for missing values:"
      ],
      "metadata": {
        "id": "Ub_Ap1Sus3yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "# Drop missing value\n",
        "Data = df.dropna()"
      ],
      "metadata": {
        "id": "wEYl11CWs6AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Formatting Date/Time\n",
        "Data['InvoiceDate'] = pd.to_datetime(Data['InvoiceDate'], format = '%m/%d/%Y %H:%M')\n",
        "\n",
        "#Strings\n",
        "Data['Description'] = Data['Description'].str.replace('.','').str.upper().str.strip()\n",
        "Data['Description'] = Data['Description'].replace('\\s+',' ',regex = True)\n",
        "Data['InvoiceNo'] = Data['InvoiceNo'].astype(str).str.upper()\n",
        "Data['StockCode'] = Data['StockCode'].str.upper()\n",
        "Data['Country'] = Data['Country'].str.upper()\n",
        "Data.head()"
      ],
      "metadata": {
        "id": "dN7uQzAcs7l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing Some Irrelevant StockCodes\n",
        "Irrelevant = Data['StockCode'].unique()\n",
        "Irrelevant.sort()\n",
        "print('Irrelevant Transactions: \\n',Irrelevant[::-1][:4])\n",
        "#Quantity and UnitPrice Summary\n",
        "Data.describe().iloc[:,:2]"
      ],
      "metadata": {
        "id": "nTuko4I9uUGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the dataset summary, it's clear that we have some unusual and irregular values in the 'UnitPrice' and 'Quantity' columns. To ensure they don't skew our analysis, we'll identify and remove these outliers.\n",
        "\n",
        "Furthermore, in the 'StockCode' variable, we've noticed that some transactions are not related to products but instead represent costs or fees associated with postage, banking, or other non-essential transactions. These non-product transactions are unnecessary for our analysis, and we'll exclude them from our dataset."
      ],
      "metadata": {
        "id": "zf-17JZku6QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy as sp\n",
        "import numpy as np\n",
        "\n",
        "# Outliers and Irrelevant Values\n",
        "# Dropping all stockcodes that contain only strings\n",
        "Data = Data[~Data['StockCode'].str.isalpha()]\n",
        "Data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Removing Outliers Based on Z-score\n",
        "Data = Data[(np.abs(sp.stats.zscore(Data['UnitPrice'])) < 3) & (np.abs(sp.stats.zscore(Data['Quantity'])) < 5)]\n"
      ],
      "metadata": {
        "id": "hs3ih-iGu3bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We've observed that some transactions involve returned products, indicated by a 'c' character at the beginning of the 'InvoiceNo' and a negative 'UnitPrice.' However, our data contains errors where purchases have negative 'UnitPrice' and vice versa. We need to correct these discrepancies and also address cases where the 'UnitPrice' is not specified.\n",
        "\n",
        "In the 'Description' attribute, there are numerous missing or incorrect values. To rectify this issue, we will:\n",
        "\n",
        "Remove transactions with no available description.\n",
        "For other missing descriptions, we'll cross-reference the 'Description' based on the product's 'StockCode' and fill in the missing values with the correct 'Description' obtained from other transactions with the same 'StockCode'. This way, we'll have more complete and accurate descriptions for our products."
      ],
      "metadata": {
        "id": "ZK5xa05OzN8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing & Incorrect Values\n",
        "Data.drop(Data[(Data.Quantity>0) & (Data.InvoiceNo.str.contains('C') == True)].index, inplace = True)\n",
        "Data.drop(Data[(Data.Quantity<0) & (Data.InvoiceNo.str.contains('C') == False)].index, inplace = True)\n",
        "Data.drop(Data[Data.Description.str.contains('?',regex=False) == True].index, inplace = True)\n",
        "Data.drop(Data[Data.UnitPrice == 0].index, inplace = True)\n",
        "\n",
        "for index,value in Data.StockCode[Data.Description.isna()==True].items():\n",
        "    if pd.notna(Data.Description[Data.StockCode == value]).sum() != 0:\n",
        "        Data.Description[index] = Data.Description[Data.StockCode == value].mode()[0]\n",
        "    else:\n",
        "        Data.drop(index = index, inplace = True)\n",
        "\n",
        "Data['Description'] = Data['Description'].astype(str)"
      ],
      "metadata": {
        "id": "dgfGSfv3y421"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our dataset, we've noticed that some rows have different 'UnitPrices' for the same products. This variation could be due to discounts, special customer conditions, or even input errors. There are instances where the same customer is charged different prices on different days, or different customers are charged different prices for the same product on the same day.\n",
        "\n",
        "For the purpose of this analysis, we'll assume that these inconsistent values are incorrect, likely resulting from input errors or human mistakes. To standardize the 'UnitPrice' for each product, we'll use the mode (most common) value as the reference price. This way, we'll have a more consistent and reliable 'UnitPrice' for our analysis."
      ],
      "metadata": {
        "id": "unA_dUZIzXbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Incorrect Prices\n",
        "StockList = Data.StockCode.unique()\n",
        "CalculatedMode = map(lambda x: Data.UnitPrice[Data.StockCode == x].mode()[0],StockList)\n",
        "StockModes = list(CalculatedMode)\n",
        "for i,v in enumerate(StockList):\n",
        "    Data.loc[Data['StockCode']== v, 'UnitPrice'] = StockModes[i]"
      ],
      "metadata": {
        "id": "k33XN6HVzSCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will add two new features: 'Final Price' calculated from 'UnitPrice' and 'Quantity,' and extract 'Month' and 'Day of the Week' from 'InvoiceDate' for time-based analysis.\n",
        "\n",
        "We'll also correct incorrect customer IDs by grouping data by 'CustomerID' and replacing any IDs associated with multiple countries with the mode (most common) country for that customer."
      ],
      "metadata": {
        "id": "zlZ9W5BPzl1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Customers with Different Countries\n",
        "Customers = Data.groupby('CustomerID')['Country'].unique()\n",
        "Customers.loc[Customers.apply(lambda x:len(x)>1)]"
      ],
      "metadata": {
        "id": "LDJs-UllzZ8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixing Duplicate CustomerIDs\n",
        "for i,v in Data.groupby('CustomerID')['Country'].unique().items():\n",
        "    if len(v)>1:\n",
        "        Data.Country[Data['CustomerID'] == i] = Data.Country[Data['CustomerID'] == i].mode()[0]\n",
        "\n",
        "#Adding Desired Features\n",
        "Data['FinalPrice'] = Data['Quantity']*Data['UnitPrice']\n",
        "Data['InvoiceMonth'] = Data['InvoiceDate'].apply(lambda x: x.strftime('%B'))\n",
        "Data['Day of week'] = Data['InvoiceDate'].dt.day_name()\n",
        "\n",
        "#Exporting Processed Data\n",
        "Data.to_csv('OnlineRetail_Cleaned.csv', date_format = '%Y-%m-%d %H:%M', index=False)"
      ],
      "metadata": {
        "id": "xNtYa4wZzrQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exploratory Data Analysis:"
      ],
      "metadata": {
        "id": "god4So2Az3Nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this section, we'll visualize the data and create informative reports and dashboards to gain a better understanding of our dataset. Here's how we'll approach it:\n",
        "\n",
        "Import the data and set the index to datetime for time series analysis.\n",
        "First, we'll identify and display the top products that were sold globally. We'll use two key indicators to measure their significance:\n",
        "\n",
        "\n",
        "2.1 In the first plot, we'll showcase the top 20 products that were purchased in the highest quantities by customers.\n",
        "In the second plot, we'll highlight the products that have generated the most revenue for us.\n",
        "These visualizations will help us gain insights into our top-performing products both in terms of sales volume and monetary benefits."
      ],
      "metadata": {
        "id": "IZlbsktK0Cyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries and the cleaned dataset\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "Data_Cleaned = pd.read_csv('/content/drive/MyDrive/DAB303/Project2/OnlineRetail_Cleaned.csv', index_col = 'InvoiceDate')\n",
        "Data_Cleaned.index = pd.to_datetime(Data_Cleaned.index, format = '%Y-%m-%d %H:%M')\n",
        "\n",
        "\n",
        "#top 20 products by quantity and finalprice\n",
        "sns.set_style('whitegrid')\n",
        "Top20Quan = Data_Cleaned.groupby('Description')['Quantity'].agg('sum').sort_values(ascending=False)[0:20]\n",
        "Top20Price = Data_Cleaned.groupby('Description')['FinalPrice'].agg('sum').sort_values(ascending=False)[0:20]\n",
        "#creating the subplot\n",
        "fig,axs = plt.subplots(nrows=2, ncols=1, figsize = (12,12))\n",
        "plt.subplots_adjust(hspace = 0.3)\n",
        "fig.suptitle('Best Selling Products by Amount and Value', fontsize=15, x = 0.4, y = 0.98)\n",
        "sns.barplot(x=Top20Quan.values, y=Top20Quan.index, ax= axs[0]).set(xlabel='Total amount of sales')\n",
        "axs[0].set_title('By Amount', size=12, fontweight = 'bold')\n",
        "sns.barplot(x=Top20Price.values, y=Top20Price.index, ax= axs[1]).set(xlabel='Total value of sales')\n",
        "axs[1].set_title('By Value', size=12, fontweight = 'bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k6hBoLuqzt9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 In this section, our focus is on understanding product returns and customer behavior related to returns. We'll first identify which products are returned most frequently by our customers, providing insights into less-satisfactory items. Additionally, we'll analyze which customers and countries have the highest number of returned items, helping us uncover trends in customer return behavior and geographic variations in return patterns."
      ],
      "metadata": {
        "id": "7u4IPfVC33lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the most returned items and the customers with the corresponding country\n",
        "ReturnedItems = Data_Cleaned[Data_Cleaned.Quantity<0].groupby('Description')['Quantity'].sum()\n",
        "ReturnedItems = ReturnedItems.abs().sort_values(ascending=False)[0:10]\n",
        "ReturnCust = Data_Cleaned[Data_Cleaned.Quantity<0].groupby(['CustomerID','Country'])['Quantity'].sum()\n",
        "ReturnCust = ReturnCust.abs().sort_values(ascending=False)[0:10]\n",
        "#creting the subplot\n",
        "fig, [ax1, ax2] = plt.subplots(nrows=2, ncols=1, figsize=(12,10))\n",
        "ReturnedItems.sort_values().plot(kind='barh', ax=ax1).set_title('Most Returned Items', fontsize=15)\n",
        "ReturnCust.sort_values().plot(kind='barh', ax=ax2).set_title('Customers with Most Returns', fontsize=15)\n",
        "ax1.set(xlabel='Quantity')\n",
        "ax2.set(xlabel='Quantity')\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2_yX5cZV2EC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 The jointplot below displays a pairwise comparison between 'UnitPrice' and 'Quantity' for purchased products. This analysis illustrates an expected trend: as the price of a product increases, the quantity sold decreases. In contrast, customers tend to purchase products in larger quantities when they have lower prices. This relationship between price and quantity sold is a common and intuitive observation in sales analysis."
      ],
      "metadata": {
        "id": "n9TKAiEE4EPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the qunatity vs unitprice\n",
        "Corr = sns.jointplot(x=\"Quantity\", y=\"UnitPrice\", data = Data_Cleaned[Data_Cleaned.FinalPrice>0], height = 7)\n",
        "Corr.fig.suptitle(\"UnitPrice and Quantity Comparison\", fontsize = 15, y = 1.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K6g95quP39YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 In the upcoming chart, we'll visualize the sales trend over the course of the year on a weekly basis. We'll achieve this by resampling our time series data to weeks and summing the values within each week. The chart will include two parts:\n",
        "\n",
        "Weekly Sales: The first chart will show the weekly sales over the year.\n",
        "\n",
        "Weekly Returns by Customers: The second chart will depict the weekly returns made by customers.\n",
        "Here are some key observations:\n",
        "\n",
        "After a noticeable dip in sales in\n",
        "\n",
        "January, there's a consistent upward trend in sales throughout the year.\n",
        "Regarding returns, they appear relatively stable throughout the year, with a minor increase, except for a spike in the second week of October."
      ],
      "metadata": {
        "id": "eH4624DR4LzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#resampling to get the weekly sales and returns\n",
        "WeeklySale = Data_Cleaned[Data_Cleaned['Quantity']>0].Quantity.resample('W').sum()\n",
        "WeeklyRet = Data_Cleaned[Data_Cleaned['Quantity']<0].Quantity.resample('W').sum().abs()\n",
        "#creating the subplot\n",
        "fig,[ax1, ax2] = plt.subplots(nrows=1,ncols=2, figsize = (15,5))\n",
        "WeeklySale.plot(ax=ax1).set(xlabel=\"Month\", ylabel=\"Quantity\")\n",
        "ax1.set_title(\"Weekly Sales Quantity\", fontsize = 15)\n",
        "WeeklyRet.plot(ax=ax2).set(xlabel=\"Month\", ylabel=\"Quantity\")\n",
        "ax2.set_title(\"Weekly Returns Quantity\", fontsize = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L5KKEZws4Iej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.5\n",
        "In the upcoming chart, we'll explore the quantity of items sold and returned across different foreign countries. As the United Kingdom dominates sales and including it might obscure insights, we'll exclude it from the chart for a clearer and more informative visualization.\n",
        "\n",
        "Here are the key observations:\n",
        "\n",
        "It appears that our products were predominantly sold in the Netherlands.\n",
        "On the other hand, the majority of returns were recorded in Ireland (EIRE). This information provides valuable insights into the distribution of sales and returns across foreign countries."
      ],
      "metadata": {
        "id": "MFx1mmlA4g1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grouping data by the countries(except UK)\n",
        "ByCountrySale = Data_Cleaned[(Data_Cleaned.Country != 'UNITED KINGDOM') & (Data_Cleaned.Quantity > 0)].groupby('Country')['Quantity'].sum()\n",
        "ByCountryRet = Data_Cleaned[(Data_Cleaned.Country != 'UNITED KINGDOM') & (Data_Cleaned.Quantity < 0)].groupby('Country')['Quantity'].sum().abs()\n",
        "#creating the subplot\n",
        "fig, [ax1,ax2] = plt.subplots(nrows=2,ncols=1,figsize=(10,14))\n",
        "ByCountrySale.plot(kind='bar', ax=ax1).set(ylabel = 'Quantity',xlabel='')\n",
        "ax1.set_title('Sales', size=12, fontweight = 'bold')\n",
        "ByCountryRet.plot(kind='bar', ax=ax2).set(ylabel = 'Quantity',xlabel='')\n",
        "ax2.set_title('Returns', size=12, fontweight = 'bold')\n",
        "plt.suptitle('Sales in Foreign Countries', fontsize = 15)\n",
        "plt.subplots_adjust(hspace = 0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NwNOA7Rm4cK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.6 Utilizing the day of the week in which items were sold, we can analyze the sales value by each day of the week. The data reveals the following pattern:\n",
        "\n",
        "Thursday shows the highest sales value.\n",
        "In contrast, Sunday records the lowest sales value.\n",
        "This information highlights the variations in sales across different days of the week and can be valuable for optimizing sales and marketing strategies."
      ],
      "metadata": {
        "id": "NVNFp5aD4yXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the pie chart\n",
        "Data_Cleaned.groupby('Day of week')['FinalPrice'].sum().plot(kind = 'pie', autopct = '%.2f%%', figsize=(7,7)).set(ylabel='')\n",
        "plt.title('Percantages of Sales Value by Day of Week', fontsize = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7KfRT5-k4oOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.7 We can identify our top customers based on the value they've contributed to the company. Additionally, we can show the countries from which these valuable customers originate. This analysis helps us recognize and appreciate our most significant customer relationships and understand their geographic distribution."
      ],
      "metadata": {
        "id": "oEPxM_txyqxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the top 10 customers by total FinalPrice\n",
        "Top10Customers = Data_Cleaned.groupby(['CustomerID', 'Country'])['FinalPrice'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Reset the index to make it a regular DataFrame\n",
        "Top10Customers = Top10Customers.reset_index()\n",
        "\n",
        "# Create a combined label for the y-axis\n",
        "Top10Customers['CustomerInfo'] = Top10Customers['CustomerID'].astype(str) + ' (' + Top10Customers['Country'] + ')'\n",
        "\n",
        "# Create the barplot with the combined y-axis label\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='FinalPrice', y='CustomerInfo', data=Top10Customers)\n",
        "plt.xlabel('Total Value')\n",
        "plt.title('Top 10 Customers by Sales Value and Country', fontsize=15)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h2lJQdQS43kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.8 In terms of future planning, it's valuable to know how many of our customers are repeat customers, which means they have made multiple purchases. In the plot below, we observe that approximately 70% of our customers fall into this category. This loyal customer base is important for business sustainability and growth.\n",
        "\n",
        "Furthermore, in the second plot, we can identify which customers from different countries have the most repeat purchases. This information can help us tailor marketing strategies and customer retention efforts more effectively."
      ],
      "metadata": {
        "id": "Sy4FaKvey0zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group customers by the number of their visits and separate them\n",
        "MostRepeat = Data_Cleaned.groupby(['CustomerID', 'Country'])['InvoiceNo'].nunique().sort_values(ascending=False)\n",
        "repeat_customers = MostRepeat[MostRepeat != 1]\n",
        "one_time_customers = MostRepeat[MostRepeat == 1]\n",
        "\n",
        "# Calculate the proportions\n",
        "repeat_customer_proportion = len(repeat_customers) / len(MostRepeat)\n",
        "one_time_customer_proportion = len(one_time_customers) / len(MostRepeat)\n",
        "\n",
        "# Create a subplot\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), gridspec_kw={'width_ratios': [3, 1]})\n",
        "plt.subplots_adjust(wspace=0.2)\n",
        "\n",
        "# First subplot: Bar plot for top repeat customers\n",
        "repeat_customers_df = repeat_customers.head(10).reset_index()\n",
        "# Create a new column 'CustomerInfo' that combines 'CustomerID' and 'Country'\n",
        "repeat_customers_df['CustomerInfo'] = repeat_customers_df['CustomerID'].astype(str) + ', ' + repeat_customers_df['Country']\n",
        "sns.barplot(x='InvoiceNo', y='CustomerInfo', data=repeat_customers_df, ax=ax1)\n",
        "ax1.set(xlabel='Number of Transactions (Repeats)', ylabel='Customer Info')\n",
        "\n",
        "# Second subplot: Pie chart for customer distribution\n",
        "proportions = [repeat_customer_proportion, one_time_customer_proportion]\n",
        "ax2.pie(proportions, labels=['Repeat Customers', 'One-time Customers'], autopct='%.2f%%')\n",
        "ax2.set(ylabel='')\n",
        "\n",
        "# Overall title\n",
        "plt.suptitle('Top Repeat Customers', fontsize=15)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gpDL9tG9yurC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.9 The plots below show the distributions of the 'Quantity' and 'UnitPrice' attributes, providing insights into the data patterns and characteristics of these features."
      ],
      "metadata": {
        "id": "onJCUOK1zAgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating distribution plots\n",
        "fig , [ax1,ax2] = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "with sns.axes_style('dark'):\n",
        "    sns.distplot(Data_Cleaned['Quantity'], ax=ax1)\n",
        "    sns.distplot(Data_Cleaned['UnitPrice'], ax=ax2)\n",
        "fig.suptitle('UnitPrice and Quantity Distribution', fontsize = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gKK2Qa0szHuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.10 In the last plot, we will use three features two show how the sales are distributed among different months and days of week. To show that, we will use seaborn's heatmap. The x-axis shows the day and the y-axis shows the month in which the items were bought. The color scale shows the total value of sales."
      ],
      "metadata": {
        "id": "bxssaCeZzPhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HM_Data = Data_Cleaned.pivot_table(index = 'InvoiceMonth',columns = 'Day of week', values = 'FinalPrice', aggfunc='sum')\n",
        "plt.figure(figsize = (10,6))\n",
        "sns.heatmap(HM_Data, cmap = 'vlag').set(xlabel='', ylabel='')\n",
        "plt.title('Sales Value per Month and Day of Week', fontsize = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r6jUV-39zdY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Association Rule Mining:\n",
        "- 3.1 Identify frequently bought products together. Use the Apriori algorithm to extract meaningful association rules.\n",
        "- 3.2 Based on the rules, suggest product bundling strategies to the retail store."
      ],
      "metadata": {
        "id": "8mUQXAUiMjLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "#importing the dataset\n",
        "Data_Cleaned = pd.read_csv('/content/drive/MyDrive/DAB303/Project2/OnlineRetail_Cleaned.csv', index_col = 'InvoiceDate')\n",
        "Data_Cleaned.index = pd.to_datetime(Data_Cleaned.index, format = '%Y-%m-%d %H:%M')\n",
        "\n",
        "#converting the data into the standard form\n",
        "Baskets = Data_Cleaned.loc[(Data_Cleaned['Quantity']>0) ,['InvoiceNo','Description','Quantity']]\n",
        "Baskets = Baskets.groupby(['InvoiceNo','Description'])['Quantity'].sum().unstack(fill_value=0)\n",
        "Baskets = (Baskets > 0)\n",
        "\n",
        "#finding frequent itemsets and association rules\n",
        "frequent_itemsets = apriori(Baskets, min_support=0.028, use_colnames=True)\n",
        "\n",
        "rules = association_rules(frequent_itemsets, metric = 'confidence', min_threshold=0.3)\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "VJy7tPsXLC6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Customer Segmentation using Clustering:"
      ],
      "metadata": {
        "id": "Qf_APZPraNCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the final part of this demonstration, we will perform customer segmentation through clustering. Customer segmentation is a valuable process for categorizing our customers into distinct groups. This segmentation can be used for various purposes, such as informing future business strategies like seasonal discounts, special offers, and more. Additionally, it allows us to assign new customers to a cluster based on their behavior.\n",
        "\n",
        "To get started, we'll import the required libraries and the dataset for this section. We'll begin by preparing the data for clustering. Our customers are divided into two main categories: those in the UK (which constitutes the majority) and those in foreign countries. We will separate them into UK and non-UK customers. Then, we'll proceed to group the data by individual customers and determine an appropriate method for aggregating the values associated with each customer. This aggregation and grouping are essential steps in preparing the data for the subsequent clustering analysis."
      ],
      "metadata": {
        "id": "zEyhs3p8arXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kmodes"
      ],
      "metadata": {
        "id": "lPpT3iQla1Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from kmodes.kprototypes import KPrototypes\n",
        "from sklearn.metrics import silhouette_score\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "#importing the dataset\n",
        "Data_Cleaned = pd.read_csv('/content/drive/MyDrive/DAB303/Project2/OnlineRetail_Cleaned.csv', index_col = 'InvoiceDate')\n",
        "Data_Cleaned.index = pd.to_datetime(Data_Cleaned.index, format = '%Y-%m-%d %H:%M')\n",
        "\n",
        "#grouping the data by customers and preparing for segmentation\n",
        "Data = Data_Cleaned.copy()\n",
        "Data['Country'] = Data['Country'].map(lambda x: 'UK' if x=='UNITED KINGDOM' else 'non-UK')\n",
        "CustomerData = Data.groupby(['CustomerID','Country'], sort=False).agg({'Quantity':'mean','UnitPrice':'mean','InvoiceNo':'nunique','Description':'nunique'})\n",
        "CustomerData.reset_index(inplace=True)\n",
        "CustomerData.columns = ['CustomerID', 'UK?', 'Average Quantity', 'Average Price', 'Repeats', 'Product Variety']\n",
        "CustomerData.head()"
      ],
      "metadata": {
        "id": "mc16v-zTPRLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before commencing the clustering process, it's wise to scale the numerical features. Scaling helps mitigate the potential negative effects of varying feature magnitudes and can also expedite the clustering process.\n",
        "\n",
        "In our customer data, we have a categorical variable representing the customer's country. To accommodate this categorical data, we've chosen the k-prototypes algorithm, which can handle both numerical and categorical variables.\n",
        "\n",
        "However, selecting the right number of clusters is crucial. We'll evaluate the clustering cost and the silhouette score to determine the optimal number of clusters. The silhouette score, in particular, provides insight into how similar data points are to their own cluster relative to other clusters, typically using Euclidean distance as a metric. This metric will guide us in selecting the appropriate number of clusters for our customer segmentation."
      ],
      "metadata": {
        "id": "w3LjqtozbVDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling the numerical features for clustering\n",
        "Scaler = StandardScaler()\n",
        "CustomerData.iloc[:,2:] = Scaler.fit_transform(CustomerData.iloc[:,2:])\n",
        "syms = CustomerData.iloc[:,0].values.astype(str)\n",
        "X = CustomerData.iloc[:,1:].values.astype(object)\n",
        "#finding the optimal cluster_number k\n",
        "for n in range(2,8):\n",
        "    kproto = KPrototypes(n_clusters = n, init = 'Cao')\n",
        "    clusters = kproto.fit_predict(X, categorical = [0])\n",
        "    silhouette = silhouette_score(X[:,1:],clusters)\n",
        "    print('number of clusters:', n)\n",
        "    print('  cost: ',kproto.cost_)\n",
        "    print('  average silhouette score: ',silhouette)"
      ],
      "metadata": {
        "id": "3fP7lR6CazYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "number_of_clusters = [2, 3, 4, 5, 6, 7]\n",
        "cost = [14261.69, 11688.49, 9374.43, 7136.42, 6353.18, 5633.88]\n",
        "silhouette_score = [0.5060, 0.5232, 0.3604, 0.3686, 0.2594, 0.2906]\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Plot cost on the primary y-axis\n",
        "ax1.set_xlabel('Number of Clusters')\n",
        "ax1.set_ylabel('Cost', color='tab:blue')\n",
        "ax1.plot(number_of_clusters, cost, color='tab:blue', marker='o', label='Cost')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax1.set_title('Clustering Metrics')\n",
        "\n",
        "# Create a secondary y-axis for silhouette score\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Average Silhouette Score', color='tab:red')\n",
        "ax2.plot(number_of_clusters, silhouette_score, color='tab:red', marker='s', label='Silhouette Score')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "# Show legends\n",
        "ax1.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C2DaR9fgbYYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}